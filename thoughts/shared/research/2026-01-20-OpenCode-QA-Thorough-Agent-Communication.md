# Research Report: OpenCode-QA-Thorough Agent-to-Agent Communication Optimization

## Metadata
- Date: 2026-01-20
- Researcher: Researcher Agent
- Topic: "OpenCode-QA-Thorough Agent-to-Agent Communication Optimization"
- Status: complete
- Coverage:
  - agent/opencode-qa-thorough.md (agent definition and output template)
  - thoughts/shared/qa/2026-01-18-Codebase-Analyzer-Agent.md (sample output)
  - AGENTS.md (thinking/answer pattern guidelines, QA workflow)
  - Background research from 2026-01-18-Codebase-Analyzer-Agent-Communication.md (industry best practices)

## Executive Summary

- The opencode-qa-thorough agent produces comprehensive QA reports but lacks thinking/answer separation despite being a primary user-facing agent
- **Gap 1 (Critical)**: No separation of analysis process (<thinking>) from final findings (<answer>), preventing debugging and token optimization for downstream consumers
- **Gap 2 (Medium)**: No structured message envelope (YAML frontmatter with correlation IDs) for multi-step workflow tracking
- **Gap 3 (Documentation)**: Agent not listed in AGENTS.md thinking/answer pattern section despite meeting criteria ("primary agents that communicate directly with users")
- Verbosity analysis: Report length scales naturally with findings (conditional sections), no fixed-verbosity problem unlike codebase-analyzer
- Highest-impact recommendation: Add <thinking>/<answer> separation to enable user debugging and QA-Planner token optimization (~15-20% overhead, ~15% savings for downstream)
- Secondary recommendation: Add YAML frontmatter message envelope for workflow correlation (~3% overhead)

## Coverage Map

**Files Inspected**:
- `agent/opencode-qa-thorough.md` (agent definition, lines 1-513)
- `thoughts/shared/qa/2026-01-18-Codebase-Analyzer-Agent.md` (sample output, 395 lines)
- `AGENTS.md` (lines 5-50 for thinking/answer pattern, QA workflow documentation)

**Background Research** (not re-verified, relied on prior analysis):
- `thoughts/shared/research/2026-01-18-Codebase-Analyzer-Agent-Communication.md` (industry best practices from 7 authoritative sources)

**Scope**:
- Complete analysis of opencode-qa-thorough output structure
- Evidence-based comparison against AGENTS.md guidelines
- Gap identification using codebase-analyzer research as baseline
- No implementation changes made (research only)

## Critical Findings (Verified, Planner Attention Required)

### Finding 1: No Thinking/Answer Separation Despite Primary User-Facing Role

**Observation**: The opencode-qa-thorough agent produces QA reports directly for users but does not separate analysis reasoning (<thinking>) from final findings (<answer>), violating the pattern documented in AGENTS.md for "primary agents that communicate directly with users".

**Direct consequence**: Users cannot inspect the agent's reasoning process (which files were read, which tools were executed, how issues were prioritized), and the downstream QA-Planner agent cannot strip reasoning tokens when parsing tasks.

**Evidence**: `agent/opencode-qa-thorough.md:176-268` (output template)
**Excerpt**:
```markdown
````markdown
```markdown
# OpenCode QA Analysis: [Target]

## Scan Metadata
[No <thinking> wrapper visible]
```
````
```

**Evidence**: `AGENTS.md:11-15` (thinking/answer pattern guideline)
**Excerpt**:
```markdown
### Thinking/Answer Separation for User-Facing Outputs

Primary agents that communicate directly with users should separate operational reasoning from actionable results using a structured three-part format:
```

**Evidence**: `AGENTS.md:25-30` (agents using pattern)
**Excerpt**:
```markdown
#### Agents Using This Pattern

- **task-executor** (`agent/task-executor.md`): Documents task execution reasoning
- **implementation-controller** (`agent/implementation-controller.md`): Documents orchestration reasoning
- **codebase-analyzer** (`agent/codebase-analyzer.md`): Documents code analysis reasoning
```

**Gap Identified**: opencode-qa-thorough is NOT listed in this section despite being a primary/all-mode agent that writes user-facing reports.

**Token Impact**: Estimated +200-300 tokens for <thinking> section (+15-20% overhead), but enables ~15% savings for QA-Planner when parsing tasks.

### Finding 2: Sample Output Shows Clean Findings But No Reasoning Trail

**Observation**: The sample QA report at `thoughts/shared/qa/2026-01-18-Codebase-Analyzer-Agent.md` contains well-structured findings with evidence (file:line + excerpts) but does not document HOW the agent arrived at its conclusions.

**Direct consequence**: If a user questions why an issue was categorized as "High Priority" vs "Medium Priority", or wonders which files the agent actually read, there is no visible audit trail in the output.

**Evidence**: `thoughts/shared/qa/2026-01-18-Codebase-Analyzer-Agent.md:215-237` (sample QA task)
**Excerpt**:
```markdown
### QA-002: Standardize Markdown List Bullet Style
- **Priority**: High
- **Category**: Validation
- **File(s)**: `agent/codebase-analyzer.md:77-209` (entire system prompt body)
- **Issue**: Mixed use of asterisk (*) and dash (-) bullet points (36 violations)
- **Evidence**: 
  ```markdown
  # Line 77 (asterisk)
      * If Function A calls Function B (imported from `./utils.ts`), use `read` on `./utils.ts`.
  ```
  markdownlint output: `MD004/ul-style Unordered list style [Expected: dash; Actual: asterisk]`
- **Recommendation**: 
  1. Use find-and-replace to change all bullet markers from `* ` to `- ` in lines 77-209
```

**Missing Information** (should be in <thinking>):
- Why is this "High Priority" instead of "Medium" or "Low"?
- What was the exact markdownlint command executed?
- How many total violations were found across all categories?
- Which other files were analyzed (comparison baseline)?

**Token Impact**: Users must trust agent's judgment without visibility into decision process.

### Finding 3: No Message Envelope for Workflow Correlation

**Observation**: The opencode-qa-thorough agent writes QA reports to files but does not include structured message metadata (correlation IDs, timestamps, message types) in YAML frontmatter.

**Direct consequence**: Cannot correlate QA reports with downstream QA-Planner actions or Implementation-Controller executions, making multi-step workflow debugging difficult.

**Evidence**: `agent/opencode-qa-thorough.md:176-185` (template start)
**Excerpt**:
```markdown
````markdown
```markdown
# OpenCode QA Analysis: [Target]

## Scan Metadata
- Date: YYYY-MM-DD
- Target: [path]
- Auditor: opencode-qa-thorough
```
````
```

**Comparison**: Other agents using thinking/answer pattern include YAML frontmatter

**Evidence**: `AGENTS.md:36` (documented benefit)
**Excerpt**:
```markdown
- **Workflow correlation via YAML frontmatter**: `message_id` and `correlation_id` enable tracing outputs across agents
```

**Gap Identified**: QA reports use Markdown section for metadata instead of YAML frontmatter, inconsistent with other primary agents.

**Token Impact**: +40-50 tokens (+3%) for message envelope, significant debugging value.

### Finding 4: Template Verbosity Scales Naturally with Findings

**Observation**: Unlike codebase-analyzer (which always returns 4 sections regardless of query complexity), the opencode-qa-thorough template has conditional sections that scale with the number of findings.

**Direct consequence**: A clean agent file with no issues produces a short report; a problematic agent produces a long report. This is optimal design and requires NO changes.

**Evidence**: `agent/opencode-qa-thorough.md:232-261` (conditional sections)
**Excerpt**:
```markdown
## Improvement Plan (For Implementor)

### QA-001: [Issue Title]
[Repeat for each issue]

## Acceptance Criteria
- [ ] All critical configuration errors resolved
[Additional criteria based on findings]

## Implementor Checklist
- [ ] QA-001: [Short title]
- [ ] QA-002: [Short title]
[etc.]
```

**Analysis**: 
- Fixed sections: Scan Metadata, Executive Summary, Automated Tool Findings, References (~30% of report)
- Conditional sections: Manual Quality Analysis (empty if no issues), Improvement Plan (scales with QA-XXX count), Acceptance Criteria, Implementor Checklist (~70% of report)

**Comparison to codebase-analyzer**: Analyzer has 4 fixed sections (100% verbosity regardless of query), causing 60-70% token waste for focused queries. QA agent does NOT have this problem.

**Token Impact**: No verbosity optimization needed; current design is efficient.

## Detailed Technical Analysis (Verified)

### Current opencode-qa-thorough Output Structure

**Template Location**: `agent/opencode-qa-thorough.md:176-268`

**Structure** (9 sections, 4 fixed + 5 conditional):

**Fixed Sections** (always present):
1. **Scan Metadata**: Date, target path, auditor, tools used
2. **Executive Summary**: Overall status, issue counts by priority
3. **Automated Tool Findings**: yamllint, markdownlint, naming conventions (subsections may be empty if tools pass)
4. **References**: Files analyzed, tool outputs, subagent delegations

**Conditional Sections** (scale with findings):
5. **Manual Quality Analysis**: Agent/skill clarity, configuration correctness, functional validation (empty if no issues)
6. **Improvement Plan**: QA-XXX tasks with priority, evidence, recommendations (empty if no issues)
7. **Acceptance Criteria**: Checkboxes for verification (empty if no issues)
8. **Implementor Checklist**: Summary of QA-XXX tasks (empty if no issues)

**Evidence**: `agent/opencode-qa-thorough.md:176-261`
**Excerpt**:
```markdown
## Automated Tool Findings

### ðŸ“‹ YAML Validation (yamllint)
- **Status**: [PASSED/FAILED]
- **Errors**: [count]

[...]

## Improvement Plan (For Implementor)

### QA-001: [Issue Title]
[Repeat for each issue]
```

**Strengths**:
- Logical organization into semantic sections
- Evidence-based findings with file:line + code excerpts (matches Researcher requirements)
- Specific, actionable recommendations (no vague language)
- Observable "Done When" conditions
- Scales naturally with findings (no fixed-verbosity waste)

**Weaknesses**:
- No <thinking>/<answer> separation (reasoning mixed or absent)
- No YAML frontmatter message envelope
- Not documented in AGENTS.md thinking/answer pattern list

### Consumer Agent Requirements Analysis

#### User/Developer (Primary Consumer)

**Role**: Reads QA report to understand code quality issues and remediation steps

**Input Requirements**:
- Clear prioritization (Critical/High/Medium/Low)
- Evidence with file:line + code excerpts
- Specific, actionable recommendations
- Observable acceptance criteria

**Current Satisfaction**: âœ… All requirements met (sample report shows clean, actionable findings)

**Gap Identified**: Cannot see HOW agent prioritized issues or WHICH files were read during analysis. If user disagrees with a finding, no reasoning trail to inspect.

**Example Use Case**: User sees QA-002 flagged as "High Priority" but thinks it should be "Low Priority" (cosmetic issue). Without <thinking> section showing prioritization reasoning, user cannot understand agent's logic.

#### QA-Planner Agent (Secondary Consumer)

**Role**: Converts QA report to implementation plan at `thoughts/shared/plans/YYYY-MM-DD-QA-[Target].md`

**Input Requirements** (inferred from workflow):
- Structured QA-XXX tasks with evidence
- Priority levels for sequencing
- Recommendations for plan instructions
- Done When criteria for verification

**Evidence**: `AGENTS.md` (QA workflow documentation)
**Excerpt**:
```markdown
- **QA Workflow**: QA agents write to `thoughts/shared/qa/` â†’ QA-Planner converts to `thoughts/shared/plans/YYYY-MM-DD-QA-[Target].md` â†’ Implementation-Controller executes fixes
```

**Current Satisfaction**: âœ… QA report provides all required structured data

**Gap Identified**: QA-Planner must parse entire report including Scan Metadata, Executive Summary, and References sections. If report included <thinking> section with tool outputs and file read logs, QA-Planner would waste tokens processing that reasoning. Separating <thinking>/<answer> would allow QA-Planner to strip thinking section.

**Token Savings**: Estimated 15% reduction in QA-Planner input tokens if thinking section can be stripped.

#### Implementation-Controller (Tertiary Consumer)

**Role**: Executes implementation plan created by QA-Planner (indirectly consumes QA report)

**Input Requirements**: None (consumes QA-Planner output, not QA report directly)

**Gap Identified**: None (no direct consumption)

### Workflow Analysis: QA Report â†’ QA-Planner â†’ Implementation-Controller

**Current Flow**:
1. opencode-qa-thorough writes `thoughts/shared/qa/YYYY-MM-DD-[Target].md`
2. User manually invokes QA-Planner with report path (workflow not automated)
3. QA-Planner reads entire report, extracts QA-XXX tasks, writes implementation plan
4. User manually invokes Implementation-Controller with plan path
5. Implementation-Controller executes tasks

**Correlation Challenge**: If multiple QA reports exist in `thoughts/shared/qa/`, and multiple implementation plans exist in `thoughts/shared/plans/`, there is no programmatic way to link them without manual filename inspection.

**Evidence**: No correlation IDs in QA report template

**Direct consequence**: Cannot build automated workflow orchestration or debugging tools that trace QA report â†’ plan â†’ execution.

**Token Impact**: +40-50 tokens for message envelope enables workflow correlation, minimal overhead for significant tooling value.

### Industry Best Practices (Background Research)

**Note**: The following findings are from prior research documented in `thoughts/shared/research/2026-01-18-Codebase-Analyzer-Agent-Communication.md`. They are included here for completeness but were NOT re-verified during this analysis.

#### Source 1: Anthropic Multi-Agent Research System (June 2025)

**Key Finding**: Structured thinking with `<thinking>` tags improves efficiency

**Excerpt from prior research**:
```markdown
Structured thinking with XML tags:
- Use `<thinking>` tags for reasoning
- Extended thinking mode improves instruction-following and efficiency
- Interleaved thinking after tool results
```

**Direct consequence**: Separating reasoning from findings enables debugging and token optimization.

#### Source 2: AGENTS.md Guidelines (Current)

**Thinking/Answer Pattern Benefits**:

**Evidence**: `AGENTS.md:32-36`
**Excerpt**:
```markdown
#### Benefits

- **Users see only actionable information**: ~30-70% reduction in visible text depending on agent type
- **Full reasoning trail available for debugging**: Inspect `<thinking>` section when outputs are unexpected
- **Consistent structure across agent outputs**: All primary agents use same format
- **Token optimization**: Consumers can strip `<thinking>` section when passing results downstream (~10% savings)
```

**Applicability to opencode-qa-thorough**:
- Agent is primary/all-mode âœ…
- Communicates directly with users âœ…
- Has downstream consumer (QA-Planner) âœ…
- Performs complex analysis requiring reasoning âœ…

**Conclusion**: opencode-qa-thorough meets ALL criteria for thinking/answer pattern adoption.

#### Source 3: Message Envelope Standards (Prior Research)

**Excerpt from prior research** (ApXML Communication Protocols):
```markdown
Message Content Requirements:
- Sender/recipient identification
- Message type (performative/intent)
- Timestamp for ordering
- Correlation ID for linking related messages
```

**Applicability to opencode-qa-thorough**:
- Multi-step workflow: QA report â†’ QA-Planner â†’ Implementation-Controller âœ…
- File-based output (not ephemeral agent response) âœ…
- Multiple reports may exist concurrently âœ…

**Conclusion**: Message envelope with correlation IDs would enable workflow tracing and debugging.

## Token Impact Analysis

**Estimated Current State** (average QA report, based on 395-line sample):

**Total**: ~1200-1500 tokens
- Fixed sections: ~400 tokens (27%)
- Conditional sections: ~800-1100 tokens (73%, scales with findings)
- No <thinking> section: 0 tokens
- No message envelope: 0 tokens

**Projected State with Recommendations**:

**Total**: ~1440-1850 tokens (+15-20%)
- Fixed sections: ~400 tokens (22%)
- Conditional sections: ~800-1100 tokens (54%)
- <thinking> section: ~200-300 tokens (14%)
- Message envelope: ~40-50 tokens (2%)

**Use Case Analysis**:

**User/Developer** (reading for understanding):
- Current: 1200-1500 tokens visible
- Optimized: 1200-1500 tokens in <answer>, 200-300 tokens in <thinking> (inspect only if needed)
- User experience: Same actionable content, optional debugging visibility
- Estimated visibility reduction: 0% (same content) but 100% debugging improvement (can inspect when needed)

**QA-Planner** (parsing tasks):
- Current: Reads entire 1200-1500 token report
- Optimized: Strips <thinking> section, reads only <answer> (1200-1500 tokens)
- Token savings: ~15% if thinking section can be programmatically stripped
- Net impact: +15-20% overhead in QA report, -15% savings in QA-Planner input

**Implementation-Controller** (indirect consumer):
- No direct consumption, no token impact

**Overall Multi-Agent Workflow Impact**:

Using Anthropic's guidance that multi-agent systems magnify efficiency gains:
- User debugging capability: +100% (from 0% to full reasoning visibility)
- QA-Planner token efficiency: -15% (if thinking stripped)
- Report generation cost: +15-20% (one-time overhead)
- Net workflow efficiency: Positive (one-time 15-20% cost enables recurring 15% savings)

## Verification Log

**Verified** (files personally read):
- `agent/opencode-qa-thorough.md:1-513`
- `thoughts/shared/qa/2026-01-18-Codebase-Analyzer-Agent.md:1-395`
- `AGENTS.md` (spot-checked lines 5-50 for thinking/answer pattern, QA workflow)

**Spot-checked excerpts captured**: Yes

**Background Research** (relied on prior analysis, not re-verified):
- `thoughts/shared/research/2026-01-18-Codebase-Analyzer-Agent-Communication.md` (industry best practices from 7 authoritative sources)

**Assumptions**:
- QA-Planner agent exists and consumes QA reports (inferred from AGENTS.md workflow description, not verified by reading QA-Planner agent definition)
- QA reports are programmatically parsed (not verified, assumed based on workflow automation intent)

## Open Questions / Unverified Claims

### Question 1: QA-Planner Agent Implementation

**Claim**: AGENTS.md documents a "QA-Planner" agent that converts QA reports to implementation plans.

**What I tried**:
- Searched AGENTS.md for "QA-Planner" reference
- Found workflow description but did not verify QA-Planner agent definition exists

**Evidence missing**: `agent/qa-planner.md` file existence and implementation details

**Implication**: If QA-Planner does not exist or uses different workflow, the token optimization benefits for downstream consumers cannot be realized.

### Question 2: Actual QA Report Token Counts

**Claim**: Estimated 1200-1500 tokens for typical QA report based on 395-line sample.

**What I tried**:
- Counted lines in sample report
- Extrapolated tokens using ~3 tokens per line heuristic

**Evidence missing**: Actual token count measured by LLM tokenizer

**Implication**: Token impact projections are estimates, not measured data. Actual overhead could be higher or lower.

### Question 3: Thinking Section Content

**Claim**: <thinking> section should include tool executions, file reads, delegation log, prioritization reasoning.

**What I tried**:
- Reviewed agent operational workflow (phases 1-5)
- Identified information types that should be logged

**Evidence missing**: User feedback on what reasoning details are most valuable for debugging

**Implication**: <thinking> section design is based on researcher judgment, not validated user requirements. May include too much or too little detail.

### Question 4: Message Envelope Schema

**Claim**: Message envelope should include message_id, correlation_id, timestamp, message_type.

**What I tried**:
- Reviewed AGENTS.md thinking/answer pattern
- Checked codebase-analyzer research for message envelope standards

**Evidence missing**: Standardized schema definition for QA report message envelope across all primary agents

**Implication**: Each agent may implement different envelope fields, reducing consistency benefits.

## Recommendations (Evidence-Based)

### Recommendation 1: Add Thinking/Answer Separation (CRITICAL)

**Change**: Wrap QA report in `<thinking>` and `<answer>` tags

**Before** (current):
```markdown
Write to `thoughts/shared/qa/YYYY-MM-DD-[Target].md` using this exact template:

````markdown
# OpenCode QA Analysis: [Target]

## Scan Metadata
- Date: YYYY-MM-DD
```
````
```

**After** (recommended):
```markdown
Write to `thoughts/shared/qa/YYYY-MM-DD-[Target].md` using this exact template:

<thinking>
Analysis Process Log:

Phase 1 - Target Identification:
- User provided: [path] OR delegated to codebase-locator
- Found targets: [list of files]

Phase 2 - Skill Loading:
- Loaded: opencode skill (version X.X)
- Extracted validation rules: [count] rules

Phase 3 - Automated Tool Execution:
- yamllint command: [exact command]
- yamllint output: [summary or "PASSED"]
- markdownlint command: [exact command]
- markdownlint output: [summary or "PASSED"]
- Custom naming checks: [results]

Phase 4 - Manual Analysis:
- Files read: [list with line ranges]
- Clarity issues found: [count]
- Configuration issues found: [count]
- Functional issues found: [count]

Phase 5 - Prioritization Reasoning:
- Critical issues: [count] (reasoning: configuration errors preventing agent loading)
- High issues: [count] (reasoning: tool permission misalignment, incorrect mode)
- Medium issues: [count] (reasoning: suboptimal settings, unclear patterns)
- Low issues: [count] (reasoning: documentation improvements, cosmetic)

Delegations Made:
- codebase-locator: [task description, scope level, files found]
- codebase-pattern-finder: [task description, patterns found]
- codebase-analyzer: [task description, depth level, findings]
</thinking>

<answer>
````markdown
# OpenCode QA Analysis: [Target]

## Scan Metadata
- Date: YYYY-MM-DD
[rest of template unchanged]
```
````
</answer>
```

**Justification**:
- Matches AGENTS.md guideline for "primary agents that communicate directly with users" (AGENTS.md:11-15)
- Enables users to inspect reasoning when findings are questioned
- Allows QA-Planner to strip <thinking> for token optimization (~15% savings)
- Provides audit trail for debugging (tool commands, file reads, delegation log)
- Industry best practice (Anthropic official recommendation)

**Implementation**:
- Update `agent/opencode-qa-thorough.md:164-174` to add <thinking> wrapper instruction
- Add explicit list of information to log in <thinking> section
- Wrap template at lines 176-268 in <answer> tags
- Update AGENTS.md:25-30 to add opencode-qa-thorough to thinking/answer pattern list

**Token Impact**: +200-300 tokens (+15-20%) per report, but enables ~15% savings for QA-Planner

**Priority**: **HIGH** - Aligns with documented agent communication patterns, significant user and workflow benefits

### Recommendation 2: Add Structured Message Envelope

**Change**: Add YAML frontmatter to QA report template

**Before** (current):
```markdown
````markdown
```markdown
# OpenCode QA Analysis: [Target]

## Scan Metadata
- Date: YYYY-MM-DD
- Target: [path]
```
````
```

**After** (recommended):
```markdown
````markdown
---
message_id: qa-2026-01-20-001
correlation_id: qa-workflow-codebase-analyzer
timestamp: 2026-01-20T14:30:00Z
message_type: QA_REPORT
qa_agent_version: 1.0
target_path: agent/codebase-analyzer.md
target_type: agent
overall_status: Conditional Pass
critical_issues: 1
high_priority_issues: 2
improvement_opportunities: 3
---

# OpenCode QA Analysis: [Target]

## Scan Metadata
- Date: YYYY-MM-DD
- Target: [path]
[rest of template unchanged]
```
````
```

**Justification**:
- Industry standard for multi-agent message passing (ApXML Communication Protocols, prior research)
- Enables workflow correlation: QA report â†’ QA-Planner â†’ Implementation-Controller
- Supports programmatic parsing and debugging tools
- Consistent with other primary agents using thinking/answer pattern
- Minimal overhead (~3%) for significant tooling value

**Implementation**:
- Update `agent/opencode-qa-thorough.md:176` to add YAML frontmatter before template
- Auto-generate message_id using timestamp + sequence
- Accept correlation_id from caller (or generate from target path)
- Include summary metrics in frontmatter for quick parsing

**Token Impact**: +40-50 tokens (+3%) per report

**Priority**: **MEDIUM** - Workflow correlation is valuable but not blocking current functionality

### Recommendation 3: Document in AGENTS.md Thinking/Answer Pattern List

**Change**: Add opencode-qa-thorough to AGENTS.md thinking/answer pattern list

**Before** (current):
```markdown
#### Agents Using This Pattern

- **task-executor** (`agent/task-executor.md`): Documents task execution reasoning
- **implementation-controller** (`agent/implementation-controller.md`): Documents orchestration reasoning
- **codebase-analyzer** (`agent/codebase-analyzer.md`): Documents code analysis reasoning
- **codebase-locator** (`agent/codebase-locator.md`): Documents file discovery reasoning
- **codebase-pattern-finder** (`agent/codebase-pattern-finder.md`): Documents pattern search reasoning
- **web-search-researcher** (`agent/web-search-researcher.md`): Documents external research reasoning
```

**After** (recommended):
```markdown
#### Agents Using This Pattern

- **task-executor** (`agent/task-executor.md`): Documents task execution reasoning
- **implementation-controller** (`agent/implementation-controller.md`): Documents orchestration reasoning
- **codebase-analyzer** (`agent/codebase-analyzer.md`): Documents code analysis reasoning
- **codebase-locator** (`agent/codebase-locator.md`): Documents file discovery reasoning
- **codebase-pattern-finder** (`agent/codebase-pattern-finder.md`): Documents pattern search reasoning
- **web-search-researcher** (`agent/web-search-researcher.md`): Documents external research reasoning
- **opencode-qa-thorough** (`agent/opencode-qa-thorough.md`): Documents QA analysis reasoning (tool execution, file reads, prioritization decisions)
```

**Justification**:
- Ensures documentation consistency
- Helps future agent developers understand pattern adoption criteria
- Makes thinking/answer pattern discoverable for opencode-qa-thorough users

**Implementation**:
- Update `AGENTS.md:25-30` to add opencode-qa-thorough entry
- Use same format as other entries (agent name, file path, brief description of reasoning logged)

**Token Impact**: No change to agent output; documentation update only

**Priority**: **LOW** - Documentation improvement, not functional change

### Recommendation 4: Add Thinking Section Content Specification

**Change**: Add explicit guidance in agent prompt for what to log in <thinking> section

**Addition to `agent/opencode-qa-thorough.md`** (before Phase 5: Plan Generation):

```markdown
### Phase 4.5: Document Analysis Process (For <thinking> Section)

Log the following information for user debugging and transparency:

1. **Target Discovery**:
   - How target was identified (user-provided path vs codebase-locator delegation)
   - Files discovered (list with line counts)

2. **Tool Execution**:
   - Exact commands executed (yamllint, markdownlint, custom checks)
   - Tool outputs (summary if passed, full output if failed)
   - Tool availability (note if any tools were unavailable)

3. **File Analysis**:
   - Which files were read (paths + line ranges)
   - Which sections were analyzed (frontmatter, system prompt, delegation patterns)

4. **Delegation Log**:
   - Subagent invocations (agent name, task description, scope/depth level)
   - Subagent responses (summary of findings, files provided)

5. **Prioritization Reasoning**:
   - Why each issue was categorized as Critical/High/Medium/Low
   - Comparison to peer agents or standards
   - References to opencode skill rules

6. **Synthesis Decisions**:
   - How findings were grouped into QA-XXX tasks
   - Why certain recommendations were chosen
   - Trade-offs considered
```

**Justification**:
- Prevents under-specified or over-specified <thinking> sections
- Ensures consistent thinking content across QA runs
- Helps users understand exactly what information will be available for debugging

**Implementation**:
- Insert new section at `agent/opencode-qa-thorough.md:164` (after Phase 4, before Phase 5)
- Reference this section in Phase 5 instruction: "Wrap analysis process log from Phase 4.5 in <thinking> tags"

**Token Impact**: +50-100 tokens in agent prompt (one-time), ensures consistent 200-300 token thinking sections in output

**Priority**: **MEDIUM** - Improves thinking section quality and consistency

## Implementation Roadmap (Suggested Sequence)

### Phase 1: High-Impact Core Changes
1. **Recommendation 1**: Add thinking/answer separation (update template with <thinking>/<answer> tags)
2. **Recommendation 4**: Add thinking section content specification (define what to log)
3. **Test**: Run opencode-qa-thorough on sample agent, verify thinking section includes expected content

**Estimated effort**: 3-5 hours
**Token impact**: +15-20% per QA report, enables 15% savings for QA-Planner

### Phase 2: Workflow Integration
4. **Recommendation 2**: Add message envelope (YAML frontmatter)
5. **Update QA-Planner** (if exists): Modify to strip <thinking> section when parsing tasks
6. **Test**: Verify correlation_id linkage between QA report and QA-Planner output

**Estimated effort**: 4-6 hours (depends on QA-Planner implementation)
**Token impact**: +3% for message envelope, -15% for QA-Planner input

### Phase 3: Documentation & Consistency
7. **Recommendation 3**: Document in AGENTS.md thinking/answer pattern list
8. **Documentation**: Update AGENTS.md QA workflow section with thinking/answer pattern details
9. **Migration guide**: Document changes for users accustomed to old QA report format

**Estimated effort**: 1-2 hours
**Token impact**: 0% (documentation only)

**Total estimated effort**: 8-13 hours
**Total token impact**:
- QA report generation: +15-20% (one-time overhead per report)
- QA-Planner consumption: -15% (recurring savings)
- User debugging capability: +100% (from none to full reasoning visibility)
- Workflow correlation: Enabled (previously impossible)

## Acceptance Criteria

For implementation to be considered complete:

1. **Thinking/Answer Separation**: QA report wrapped in `<thinking>` and `<answer>` tags
2. **Thinking Content**: <thinking> section includes all 6 information categories (target discovery, tool execution, file analysis, delegation log, prioritization reasoning, synthesis decisions)
3. **Message Envelope**: YAML frontmatter includes message_id, correlation_id, timestamp, message_type, target info, summary metrics
4. **Template Consistency**: <answer> section uses existing template structure (no changes to findings format)
5. **Documentation**: AGENTS.md thinking/answer pattern list includes opencode-qa-thorough entry
6. **Backward Compatibility**: <answer> section content matches current QA report format (users see same findings)
7. **Verification**: Sample QA run produces report with both sections, thinking section includes expected content

## References

**Files Verified**:
- `agent/opencode-qa-thorough.md:1-513` (agent definition and template)
- `thoughts/shared/qa/2026-01-18-Codebase-Analyzer-Agent.md:1-395` (sample QA report)
- `AGENTS.md` (spot-checked lines 5-50)

**Background Research** (relied on prior analysis):
- `thoughts/shared/research/2026-01-18-Codebase-Analyzer-Agent-Communication.md` (industry best practices, 7 authoritative sources)
